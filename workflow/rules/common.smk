import fnmatch
import os
import re
from pathlib import Path
from threading import Lock
from typing import Any, Dict, Iterable, Mapping

from snakemake.remote import AbstractRemoteProvider
from snakemake.utils import validate


#### Configuration ####
validate(config, schema="../schemas/config.schema.yml")  # also sets default values

#### Environment configuration function - call at workflow start ####
def configure_environment():
    shell.prefix("set -o pipefail; ")

    if workflow.use_singularity:
        # If workflow is configured to access local storage (i.e., 'source' or
        # 'destination' have 'type' == 'local'), bind mount the relevant root_path
        # into container.
        # Ideally we want to mount the source in read-only mode.
        # To avoid making the working directory read-only should it be inside
        # or the same path as the working directory, we check for this case
        # and if true we mount read-write.

        # To find the directory we have to make sure it exists
        if config['destination']['type'] == 'local':
            dest_path = Path(config['destination']['root_path'])
            dest_path.mkdir(parents=True, exist_ok=True)

        def mount_fs(cfg: Dict[str, Any], prefer_rw: bool):
            if cfg['type'] != 'local':
                return

            path = Path(cfg['root_path']).resolve(strict=True)
            work_dir = Path.cwd()
            if prefer_rw or path == work_dir or path in work_dir.parents:
                mount_options = 'rw'
            else:
                mount_options = 'ro'
            workflow.singularity_args += ' '.join([
                f" --bind {path}:{path}:{mount_options}"])

        mount_fs(config['source'], prefer_rw=False)
        mount_fs(config['destination'], prefer_rw=True)

##### Helper functions #####

def create_remote_provider(remote_config: Mapping[str, str]) -> AbstractRemoteProvider:
    """
    Create a snakemake remote provider Based on the configured type.
    Returns None if the type is "local".
    """
    # LP: The snakemake.remote.AutoRemoteProvider seems like the ideal solution
    # for the problem of easily mapping a remote type to a concrete
    # RemoteProvider class.  However, as of snakemake v6.12.3 I was not able to
    # get it work.  So, we provide our own implementation here.
    import importlib

    ProviderMap = {
        "azblob": "AzBlob",
        "dropbox": "dropbox",
        "ega": "EGA",
        "ftp": "FTP",
        "gfal": "gfal",
        "gridftp": "gridftp",
        "gs": "GS",
        "http": "HTTP",
        "irods": "iRODS",
        "ncbi": "NCBI",
        "s3": "S3",
        "sftp": "SFTP",
        "webdav": "webdav",
        "xrootd": "XRootD",
    }

    remote_type = remote_config["type"].lower()
    if remote_type == "local":
        return None

    module_name = f"snakemake.remote.{ProviderMap[remote_type]}"
    provider_module = importlib.import_module(module_name)
    return provider_module.RemoteProvider(**remote_config["connection"])


# Create the remote providers.  If RProvider is None, then paths are local.
DestRProvider = create_remote_provider(config["destination"])
SourceRProvider = create_remote_provider(config["source"])


def _get_remote(remote_type: str, path: str) -> AbstractRemoteProvider:
    if remote_type not in ("source", "destination"):
        raise ValueError(f"Invalid remote_type '{remote_type}'. Expected 'source' or 'destination'")

    remote_config = config[remote_type]
    full_path = Path(remote_config["root_path"]) / path

    if remote_type == 'destination' and DestRProvider is not None:
        return DestRProvider.remote(str(full_path), **remote_config["connection"])
    elif remote_type == 'source' and SourceRProvider is not None:
        return SourceRProvider.remote(str(full_path), **remote_config["connection"])
    # else
    return full_path


def get_source_remote(path: str) -> AbstractRemoteProvider:
    return _get_remote('source', path)


def get_dest_remote(path: str) -> AbstractRemoteProvider:
    return _get_remote('destination', path)


def get_all_demangled_names() -> Iterable[str]:
    """
    Returns a list with all the decrypted data file names
    from the index generated by checkpoint rule decrypt_index.

    Can only be used in `input:` sections as it accesses checkpoints.
    """
    with checkpoints.decrypt_index.get().output.index.open() as f:
        return [line.split("\t", 2)[1] for line in f]


def _cache_index() -> None:
    class CacheItem:
        def __init__(self, mangled_name, crypt_chksum):
            self.mangled_name = mangled_name
            self.crypt_chksum = crypt_chksum

    global _gRenameIndexCache
    with _gRenameIndexLock:
        if _gRenameIndexCache is None:
            with checkpoints.decrypt_index.get().output.index.open() as f:
                # each line in the file is a tab-separated tuple (uuid4 name, original name, encrypted file checksum).
                tmp = dict()
                for line in f:
                    fields = line.rstrip("\n").split("\t")
                    tmp[fields[1]] = CacheItem(fields[0], fields[2])
            _gRenameIndexCache = tmp


def get_encrypted_item_checksum(name: str) -> str:
    """
    Can only be used in `input:` sections as it accesses checkpoints.
    """
    _cache_index()
    global _gRenameIndexCache
    with _gRenameIndexLock:
        return _gRenameIndexCache[name].crypt_chksum


def get_mangled_file_name(name: str) -> str:
    """
    Can only be used in `input:` sections as it accesses checkpoints.
    """
    _cache_index()
    global _gRenameIndexCache
    with _gRenameIndexLock:
        return _gRenameIndexCache[name].mangled_name


class FilterPattern:
    def __init__(self, type: str, pattern: str):
        if type not in ('include', 'exclude'):
            raise ValueError(f"Bad filter type {type}. Expected 'include' or 'exclude'")
        self._include = 'include' == type
        self._pattern = re.compile(fnmatch.translate(pattern))

    def keep_file(self, name: str) -> bool:
        if self._pattern.fullmatch(name) is not None:
            return self._include
        return None

    def __repr__(self) -> str:
        return f"FilterPattern(include={self._include}, pattern={self._pattern})"


def get_data_file_names() -> Iterable[str]:
    if 'filters' in config and config['filters']:
        filters = [ FilterPattern(f['type'], f['pattern']) for f in config['filters'] ]
    else:
        filters = []

    def keep_file(name) -> bool:
        for f in filters:
            k = f.keep_file(name)
            if k is not None:
                return k
        return True

    def dest_name(old_name: str) -> str:
        # Remove the trailing .c4gh extension.  If the extension is not
        # .c4gh then we leave it. It can help make the workflow easier to adapt
        # to not-encrypted files.
        head, tail = os.path.splitext(old_name)
        return head if tail == '.c4gh' else old_name

    destination_names = [q
                         for q in (dest_name(p) for p in get_all_demangled_names())
                         if keep_file(q)]
    return destination_names


##### Module-level name index cache #####

# The file renaming index is produced in a file,
# but then we need to look it up as a the workflow DAG is computed.  Rather
# than having to read the entire file for each look-up, we cache it in the
# module-level variable _gRenameIndexCache defined below.
#
# I don't know whether it's thread safe to access a global structure like this
# in snakemake rules.  Since I'm in doubt we'll protect write accesses with a
# threading.Lock

_gRenameIndexLock = Lock()
_gRenameIndexCache = None
